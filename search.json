[
  {
    "objectID": "vector_ball/report.html",
    "href": "vector_ball/report.html",
    "title": "Vector Ball Report",
    "section": "",
    "text": "TODO:\n\nBasic game architecture\nInteraction of model, view, database\nData querying\nMove from KDTree -&gt; CozoDB\n\n\n\n\nA part of this project which snuck up on me, was acquiring and preparing suitable data for the game. When first conceptualizing the design of the game I forgot about how the important the text data used to create the word embeddings would be. My first attempts to supply the game with enough words to keep it interesting was through a wordlist here, which I found through some light googling. After trying a few rounds of the game I realized that there were many problems with this as a dataset, namely:\n\nThe wordlist contains multiple versions of a word such as plurals\nThere are many obscure words\nStopwords are included\n\nFrom here I wanted to pursue two directions, find more/better data, and create a routine for cleaning a wordlist dataset. In the pursuit of the first goal I found high quality in the form of Desi Quintans’ “The Great Noun List”, which contained 6,775 of the most frequent english nouns. This list was rather useful as it contained an already thoroughly cleaned dataset of high quality words perfectly suitable for this game. I also found the google-10000-english repo which after some processing seemed to have a high overlap with the original dataset. \nFor the second point of pursuit, I came up with a short plan for cleaning the dataset. In order:\n\nCombine all wordlists into a single list by adding them to a set, thereby removing any duplicates\nRemove stopwords (with NLTK)\nRemove words that are 3 letters or shorter\nLemmatize words (with WordNet)\nPlace words into a set again to remove duplicates formed during lemmatization\n\nThis strategy seemed to reduce previous problems and create a high quality list of words for the game. Culling words 3 letters or shorter was a bit aggressive at it removed words like “cat” and “boy”, but thankfully the high quality noun list found earlier was able to supply these without any worry of contaminating the dataset.\nAt this point I felt I had a good dataset to use, but I forgot about one important thing: words have multiple meanings. How I got to this point without remembering this simple fact is both beyond me and somewhat unsurprising. This acknowledgement comes with both many great benefits and problems. Having additional meanings for each word means that we can greatly multiply the current dataset of words. However we must also now worry about managing all these extra definitions and not adding any noise to the dataset. Additionally, we also need a good source for these definitions\nAfter doing some searching, I found\n\n\nAfter reviewing some\n\n\n\n\nTODO:\n\nHow to make it fun (Data quality plays a big part in gameplay here)\nDifficulty thoughts (Easier &lt;– More Information — Less Information –&gt; Harder)"
  },
  {
    "objectID": "vector_ball/report.html#architecture",
    "href": "vector_ball/report.html#architecture",
    "title": "Vector Ball Report",
    "section": "",
    "text": "TODO:\n\nBasic game architecture\nInteraction of model, view, database\nData querying\nMove from KDTree -&gt; CozoDB"
  },
  {
    "objectID": "vector_ball/report.html#data-acquisition",
    "href": "vector_ball/report.html#data-acquisition",
    "title": "Vector Ball Report",
    "section": "",
    "text": "A part of this project which snuck up on me, was acquiring and preparing suitable data for the game. When first conceptualizing the design of the game I forgot about how the important the text data used to create the word embeddings would be. My first attempts to supply the game with enough words to keep it interesting was through a wordlist here, which I found through some light googling. After trying a few rounds of the game I realized that there were many problems with this as a dataset, namely:\n\nThe wordlist contains multiple versions of a word such as plurals\nThere are many obscure words\nStopwords are included\n\nFrom here I wanted to pursue two directions, find more/better data, and create a routine for cleaning a wordlist dataset. In the pursuit of the first goal I found high quality in the form of Desi Quintans’ “The Great Noun List”, which contained 6,775 of the most frequent english nouns. This list was rather useful as it contained an already thoroughly cleaned dataset of high quality words perfectly suitable for this game. I also found the google-10000-english repo which after some processing seemed to have a high overlap with the original dataset. \nFor the second point of pursuit, I came up with a short plan for cleaning the dataset. In order:\n\nCombine all wordlists into a single list by adding them to a set, thereby removing any duplicates\nRemove stopwords (with NLTK)\nRemove words that are 3 letters or shorter\nLemmatize words (with WordNet)\nPlace words into a set again to remove duplicates formed during lemmatization\n\nThis strategy seemed to reduce previous problems and create a high quality list of words for the game. Culling words 3 letters or shorter was a bit aggressive at it removed words like “cat” and “boy”, but thankfully the high quality noun list found earlier was able to supply these without any worry of contaminating the dataset.\nAt this point I felt I had a good dataset to use, but I forgot about one important thing: words have multiple meanings. How I got to this point without remembering this simple fact is both beyond me and somewhat unsurprising. This acknowledgement comes with both many great benefits and problems. Having additional meanings for each word means that we can greatly multiply the current dataset of words. However we must also now worry about managing all these extra definitions and not adding any noise to the dataset. Additionally, we also need a good source for these definitions\nAfter doing some searching, I found\n\n\nAfter reviewing some"
  },
  {
    "objectID": "vector_ball/report.html#game-design",
    "href": "vector_ball/report.html#game-design",
    "title": "Vector Ball Report",
    "section": "",
    "text": "TODO:\n\nHow to make it fun (Data quality plays a big part in gameplay here)\nDifficulty thoughts (Easier &lt;– More Information — Less Information –&gt; Harder)"
  },
  {
    "objectID": "ideas.html",
    "href": "ideas.html",
    "title": "Data Project Ideas",
    "section": "",
    "text": "Data Project Ideas\n\ngzip embeddings\nRemake Word2Vec\nRebuild BERT?\nSports Statistics\n\nNFL Analytics With R\n\nEncyclopedia of Activation Functions\n\nCreate plots and explanations of as many activation functions as I can find"
  },
  {
    "objectID": "self_organizing_map/som.html",
    "href": "self_organizing_map/som.html",
    "title": "Self Organizing Maps",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport einops as ein\nimport matplotlib.pyplot as plt\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML"
  },
  {
    "objectID": "self_organizing_map/som.html#data",
    "href": "self_organizing_map/som.html#data",
    "title": "Self Organizing Maps",
    "section": "Data",
    "text": "Data\nLet’s start by loading our data. We’ll be using data about Iris flowers\n\ndf = pd.read_csv(\"Iris.csv\", index_col='Id')\n\n\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\nId\n\n\n\n\n\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n146\n6.7\n3.0\n5.2\n2.3\nIris-virginica\n\n\n147\n6.3\n2.5\n5.0\n1.9\nIris-virginica\n\n\n148\n6.5\n3.0\n5.2\n2.0\nIris-virginica\n\n\n149\n6.2\n3.4\n5.4\n2.3\nIris-virginica\n\n\n150\n5.9\n3.0\n5.1\n1.9\nIris-virginica\n\n\n\n\n150 rows × 5 columns\n\n\n\nSince we’re going to be doing a lot of vector math later using these measurements, let’s convert them into vectors right away.\n\ndf[\"vector\"] = df.apply(\n    lambda r: np.array(\n        r[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]],\n        dtype=np.float32,\n    ),\n    axis=1,\n)\n\n\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nvector\n\n\nId\n\n\n\n\n\n\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n[5.1, 3.5, 1.4, 0.2]\n\n\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n[4.9, 3.0, 1.4, 0.2]\n\n\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n[4.7, 3.2, 1.3, 0.2]\n\n\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n[4.6, 3.1, 1.5, 0.2]\n\n\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n[5.0, 3.6, 1.4, 0.2]\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n146\n6.7\n3.0\n5.2\n2.3\nIris-virginica\n[6.7, 3.0, 5.2, 2.3]\n\n\n147\n6.3\n2.5\n5.0\n1.9\nIris-virginica\n[6.3, 2.5, 5.0, 1.9]\n\n\n148\n6.5\n3.0\n5.2\n2.0\nIris-virginica\n[6.5, 3.0, 5.2, 2.0]\n\n\n149\n6.2\n3.4\n5.4\n2.3\nIris-virginica\n[6.2, 3.4, 5.4, 2.3]\n\n\n150\n5.9\n3.0\n5.1\n1.9\nIris-virginica\n[5.9, 3.0, 5.1, 1.9]\n\n\n\n\n150 rows × 6 columns\n\n\n\nNow that we have the data prepared, let’s check out what our data looks like when plotted.\n\n\nCode\nfig, ax = plt.subplots(2, figsize=(8, 10))\n\nsb.scatterplot(\n    df,\n    x=\"SepalLengthCm\",\n    y=\"SepalWidthCm\",\n    hue=\"Species\",\n    ax=ax[0],\n)\n\n_ = sb.scatterplot(\n    df,\n    x=\"PetalLengthCm\",\n    y=\"PetalWidthCm\",\n    hue=\"Species\",\n    ax=ax[1],\n)\n\n\n\n\n\nFigure 1: Irises by sepal and petal measurements\n\n\n\n\nFrom these plots it appears that each of the three different species have a fairly distinct cluster."
  },
  {
    "objectID": "self_organizing_map/som.html#building-a-self-organizing-map-step-by-step",
    "href": "self_organizing_map/som.html#building-a-self-organizing-map-step-by-step",
    "title": "Self Organizing Maps",
    "section": "Building a Self Organizing Map Step by Step",
    "text": "Building a Self Organizing Map Step by Step\nThere are roughly 5 steps in fitting an SOM.\n\nSample a vector\nCalculate deltas between the sample and each map node\nFind the best matching unit (BMU)\nScale the deltas using a neighbor function\nApply the scaled deltas to the map nodes\n\nWe’ll go over each step and construct them one by one. Sampling a vector from some dataset is easy enough, so we’ll skip to the fun stuff!\n\nInitializing the weights\nBefore we can do anything, we have to create the map itself. All this takes is initializing an H x W x F tensor, where F is the feature dimension of our sample vectors.\nThere’s a variety of ways we could select the initial weight values. For now we’ll initialize the weights by sampling from normal distributions based on the mean and standard deviation of each feature. This will ensure that our starting weights are at least somewhat similar to our data.\n\ndef create_map(height: int, width: int, features: int, data: np.ndarray | None = None):\n    if isinstance(data, np.ndarray):\n        mean = data.mean(axis=0)\n        dev = data.std(axis=0) * 3\n\n    else:\n        mean = 0\n        dev = 1.0\n\n    return np.random.normal(mean, dev, size=(height, width, features))\n\n\nmap = create_map(10, 10, 4, np.stack(df['vector']))\n\n\n\nCode\n_ = sb.heatmap(\n    map.mean(axis=2),\n    annot=True,\n    yticklabels=[],\n    xticklabels=[],\n)\n\n\n\n\n\nFigure 2: Average weight values of our newly initialized self organizing map\n\n\n\n\nWe have a map! With this done we can get started with the interesting stuff.\n\n\nDelta\nThe first step of fitting is to find the difference between each map node and a sampled vector. To start off let’s grab a random vector from our data.\n\nsample = df[\"vector\"].sample(1).iloc[0]\n\n\n\nCode\n_ = sb.heatmap(\n    sample[:, None],\n    annot=True,\n    yticklabels=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n    xticklabels=[],\n    vmin=0,\n    vmax=np.stack(df[\"vector\"]).max(),\n)\n\n\n\n\n\nFigure 3: A random sample vector from our Iris data\n\n\n\n\nNext, let’s find the difference (or delta) between this sample vector and each map node.\n\ndef map_delta(map: np.ndarray, sample: np.ndarray) -&gt; np.ndarray:\n    delta = sample - ein.rearrange(map, \"h w f -&gt; (h w) f\")\n    delta = ein.rearrange(delta, \"(h w) f -&gt; h w f\", h=map.shape[0])\n\n    return delta\n\n\n\nCode\n_ = sb.heatmap(\n    map_delta(map, sample).mean(axis=2),\n    annot=True,\n    yticklabels=[],\n    xticklabels=[],\n)\n\n\n\n\n\nFigure 4: Average delta between random sample and each map node\n\n\n\n\n\n\nBest Matching Unit\nNow just finding the delta is good, but we don’t want to change the entire map at once. Instead we’ll strategically apply the most of change to specific regions. To do so we’ll need to find the map node that is the most similar to our sample. This node is what’s known as the Best Matching Unit (BMU).\nThere’s a variety of ways to compare two vectors, but we’ll be using Euclidean Distance. \n\ndef get_bmu(map: np.ndarray, vector: np.ndarray):\n    map_view = ein.rearrange(map, \"h w f -&gt; f (h w)\")\n\n    if len(vector.shape) &lt; 2:\n        vector = vector[:, None]\n\n    delta = map_view - vector\n    nearest = np.sqrt(np.einsum(\"ij, ij -&gt; j\", delta, delta))\n\n    return np.unravel_index(nearest.argsort().argmin(), map.shape[:2])\n\nidx = get_bmu(map, df[\"vector\"].iloc[0])\n\nLet’s take a look at how our sample compares to its BMU:\n\n\nCode\n_ = sb.heatmap(\n    np.stack([sample, map[idx]], axis=1),\n    yticklabels=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n    xticklabels=[\"Sample\", f\"BMU (Map Node {idx[0]}, {idx[1]})\"],\n    vmin=0,\n    vmax=np.stack([sample, map[idx]], axis=1).max(),\n    annot=True,\n)\n\n\n\n\n\nFigure 5: Our sampled vector and its closest map node side-by-side\n\n\n\n\n\n\nNeighbor Function\nThings are coming together! The last key ingredient is the Neighbor Function. This is a function that will provide a scaling coefficient for each map node based on its distance to the BMU. With this we can not only apply the most change to the BMU, but gradually decrease how much we’re change the rest of the map as we move away from the BMU.\nThe goal is to have map nodes close to each other be similar to eachother in the same way that our sample is similar to other datapoints.\nNow then, if we went ahead and made a Neighbor Function that took in the indices of a node and the indices of the BMU and found how much it should be scaled, it would work but it would be very slow. Instead, we can create all of the scales at once so we can apply changes to the entire map at once!\n\ndef neighbor_scale(index: np.ndarray, map_shape: tuple[int, int]) -&gt; np.ndarray:\n    i, j = np.indices(map_shape, sparse=True)\n\n    scale = np.power(2, np.abs(i - index[0]) + np.abs(j - index[1]))\n    # scale = scale * 1\n    scale[index] = 1\n    scale = 1 / scale\n\n    return scale\n\nLet’s try visualizing this with a heatmap to understand what’s happening a little better.\n\n\n\n\n\nFigure 6: Scaling values for nodes around the BMU\n\n\n\n\nLooking at the map we can see that the scale of the BMU index is 1, meaning we’ll be applying the maximum amount of change at that node. As we move further away from the BMU the scales rapidly decrease, meaning the changes happening in the rest of the map will be increasingly small until they’re very tiny.\nWith this we can change the BMU the most, and each of its neighbors by lesser amounts based on how far they are from the BMU\nThis marks off two critical pieces of fitting a self organizing map!\n\n\nReviewing a Fitting Step\nNow that we have each ingredient for fitting, let’s bring them together to create a fitting step.\n\ndef fit_to_sample(map: np.ndarray, sample: np.ndarray, lr: float):\n    bmu_idx = get_bmu(map, sample)\n    scale = neighbor_scale(bmu_idx, map.shape[:2])\n\n    delta = map_delta(map, sample)\n\n    return delta * np.expand_dims(scale, -1) * lr\n\nWith each of the individual steps now figured out, it should be helpful to create a visualization of everything together now.\n\nNote: The delta applied to the map will also be scaled by a learning rate (lr), but for visualization we’ll leave it at 1.0\n\n\n\nCode\nfig, ax = plt.subplots(3, 2, figsize=(10.3, 9))\n\nmean_delta = map_delta(map, sample).mean(axis=2)\nscale = neighbor_scale(idx, (10, 10))\nmean_scaled_delta = fit_to_sample(map, df[\"vector\"].iloc[0], 1.0).mean(axis=2)\nbefore = map.mean(axis=2)\nafter = (map + fit_to_sample(map, df[\"vector\"].iloc[0], 1.0)).mean(axis=2)\n\nmax, min = 6, 0\ncommon = {\n    \"xticklabels\" : [],\n    \"yticklabels\" : [],\n    \"square\" : True,\n}\n\n_ = sb.heatmap(\n    sample[:, None],\n    ax=ax[0, 0],\n    annot=True,\n    yticklabels=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n    xticklabels=[],\n    square=True,\n    vmin=0,\n    vmax=np.stack(df[\"vector\"]).max(),\n).set_title(\"Sample Vector\")\n\nsb.heatmap(\n    mean_delta,\n    ax=ax[0, 1],\n    vmin=-3,\n    vmax=3,\n    **common\n).set_title(\"Mean Delta\")\n\nsb.heatmap(\n    scale,\n    ax=ax[1, 0],\n    vmin=0,\n    vmax=1,\n    **common\n).set_title(\"Neighbor Scaling\")\n\nsb.heatmap(\n    mean_scaled_delta,\n    ax=ax[1, 1],\n    vmin=-3,\n    vmax=3,\n    **common\n).set_title(\"Mean Scaled Delta\")\n\nsb.heatmap(\n    before,\n    ax=ax[2, 0],\n    vmin=min,\n    vmax=max,\n    **common\n).set_title(\"Before Step\")\n\n_ = sb.heatmap(\n    after,\n    ax=ax[2, 1],\n    vmin=min,\n    vmax=max,\n    **common\n).set_title(\"After Step\")\n\n\n\n\n\nFigure 7: Each part of a fitting step laid out\n\n\n\n\nNow we can see a single step of the complete process in front of us!"
  },
  {
    "objectID": "self_organizing_map/som.html#bringing-it-all-together",
    "href": "self_organizing_map/som.html#bringing-it-all-together",
    "title": "Self Organizing Maps",
    "section": "Bringing it All Together",
    "text": "Bringing it All Together\nNow that we’ve seen what a single fitting step looks like, we need to bring it all together in a training loop so we can apply it to the rest of our dataset.\n\ndef fit_epoch(\n    map,\n    data,\n    lr,\n):\n    new_map = np.copy(map)\n    for x in data:\n        new_map += fit_to_sample(new_map, x, lr)\n\n    return new_map\n\nThat’s pretty much it! We’ve done so much of the work previously that a training loop for a single epoch is this simple. Now if we wanted to try things like sampling data differently, training for multiple epochs, dynamically adjusting the learning rate, it’d all be pretty easy to accomplish by modifying this simple loop and utilizing the tools we’ve built. For now though, let’s see what a full epoch of training looks like.\n\n\nCode\ngrid_kws = {'wspace': 0.2}\n# fig, (ax, cbar_ax) = plt.subplots(1, 2, gridspec_kw = grid_kws, figsize = (5, 4))\n\nfig, ax = plt.subplots(4, 2, figsize=(8, 8))\nfig.tight_layout()\n\nanim_map = map.copy()\nanim_data = df['vector'].sample(frac=1).tolist()\nanim_data = anim_data * 3\nanim_lr = 0.3\n\ndef animate(i):\n    global anim_map\n\n    sample = anim_data[i]\n\n    bmu_idx = get_bmu(anim_map, sample)\n    mean_delta = np.abs(map_delta(anim_map, sample)).mean(axis=2)\n    scale = neighbor_scale(bmu_idx, (10, 10))\n    mean_scaled_delta = np.abs(fit_to_sample(anim_map, sample, anim_lr).mean(axis=2))\n    after = anim_map + fit_to_sample(anim_map, sample, anim_lr)\n\n    anim_map += fit_to_sample(anim_map, sample, anim_lr)\n\n    common = {\n        \"xticklabels\" : [],\n        \"yticklabels\" : [],\n        \"square\" : True,\n        \"cbar\": False,\n    }\n\n    sb.heatmap(\n        np.stack([sample, anim_map[bmu_idx]], axis=1),\n        ax=ax[0, 0],\n        **common,\n    ).set_title(\"Sample & BMU\")\n\n    sb.heatmap(\n        mean_delta,\n        ax=ax[0, 1],\n        vmin=0,\n        vmax=3,\n        **common\n    ).set_title(\"Mean Absolute Delta\")\n\n    sb.heatmap(\n        scale,\n        ax=ax[1, 0],\n        vmin=0,\n        vmax=1,\n        **common,\n    ).set_title(\"Neighbor Scaling\")\n\n    sb.heatmap(\n        mean_scaled_delta,\n        ax=ax[1, 1],\n        vmin=0,\n        vmax=1,\n        **common\n    ).set_title(\"Scaled Delta\")\n\n    sb.heatmap(\n        after[:, :, 0],\n        ax=ax[2, 0],\n        **common\n    ).set_title(\"Sepal Length\")\n\n    sb.heatmap(\n        after[:, :, 1],\n        ax=ax[2, 1],\n        **common\n    ).set_title(\"Sepal Width\")\n\n    sb.heatmap(\n        after[:, :, 2],\n        ax=ax[3, 0],\n        **common\n    ).set_title(\"Petal Length\")\n\n    sb.heatmap(\n        after[:, :, 3],\n        ax=ax[3, 1],\n        **common\n    ).set_title(\"Petal Width\")\n\nani = FuncAnimation(fig=fig, func=animate, frames=len(anim_data), interval=1)\n\nplt.close()\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\nFigure 8: Full animation of one training epoch\n\n\n\nNow let’s observe what our data looks like when we use the map to reduce the features. Let’s upgrade our get_bmu function so it can find the BMU for many samples at once.\n\ndef get_bmu(map, vector):\n\n    if len(vector.shape) &lt; 2:\n        vector = vector[:, None]\n\n    map_view = ein.rearrange(map, \"h w f -&gt; f (h w)\")\n\n    delta = map_view[:, None, :] - vector[:, :, None]\n\n    nearest = np.sqrt(np.einsum(\"i...j, i...j -&gt; ...j\", delta, delta))\n\n    bmu = np.stack(\n        np.unravel_index(nearest.argsort(axis=1).argmin(axis=1), fitted.shape[:2])\n    ).T\n\n    if bmu.shape[0] == 1:\n        return bmu[0]\n    else:\n        return bmu\n\nNow let’s try using our newly fitted map to perform feature reduction on our data\n\n\nCode\nfitted = anim_map.copy()\n\nindices = get_bmu(fitted, np.stack(df[\"vector\"], axis=1))\n\n_ = sb.scatterplot(\n    data=df.assign(\n        x=indices[:, 0],\n        y=indices[:, 1],\n    ),\n    x=\"x\",\n    y=\"y\",\n    hue='Species',\n    style='Species'\n)\n\n\n\n\n\nFigure 9: Reduction of Iris data\n\n\n\n\nWe should see that the reduced data appears roughly similar to the distributions we saw when we looked at the petal and sepal measurements seperately."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS Toys",
    "section": "",
    "text": "Self Organizing Maps\n\n\n\n\n\n\n\nunsupervised\n\n\nfeature-reduction\n\n\nvisualization\n\n\n\n\nExploring how self organizing maps are created\n\n\n\n\n\n\nJun 3, 2024\n\n\nPax Newman\n\n\n\n\n\n\nNo matching items"
  }
]